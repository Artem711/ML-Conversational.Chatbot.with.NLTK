{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333d8005",
   "metadata": {},
   "source": [
    "## Theory: Introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7112ea91",
   "metadata": {},
   "source": [
    "## Theory: Importance of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d53071",
   "metadata": {},
   "source": [
    "## Theory: Introduction to NLTK for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589b2e3",
   "metadata": {},
   "source": [
    "## Practical: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce45fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful libraries\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d06493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download_gui()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2930c",
   "metadata": {},
   "source": [
    "Using the NLTK GUI you must download the following:\n",
    "- stopwords from 'Corpora'\n",
    "- averaged_perceptron_tagger from 'All packages'\n",
    "- wordnet from 'All packages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99151645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regular expression\n",
    "import os # operating system\n",
    "import csv\n",
    "import random\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f0f9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the setting of our Python Notebook\n",
    "\n",
    "## Get multiple outputs in the same cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "## Display all rows and columns of a dataframe instead of a truncated version\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ce565a",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33da30b",
   "metadata": {},
   "source": [
    "Below, we have two sentences:\n",
    "- First sentence is just a standard sentence in english language. \n",
    "- Second sentence is another standard sentence in english language. Both of the sentences have punctuation (full stops, commas, brackets, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "182e922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The Big brown fox jumped over a lazy dog.\"\n",
    "sentence2 = \"This is particularly important in today's world where we are swamped with unstructured natural language data on the variety of social media platforms people engage in now-a-days (note -  now-a-days in the decade of 2010-2020)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1530236",
   "metadata": {},
   "source": [
    "Now, we will apply various NLP technique on these two sentence and see the effects that they produce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13ae2495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n",
      "the big brown fox jumped over a lazy dog.\n",
      "\n",
      "this is particularly important in today's world where we are swamped with unstructured natural language data on the variety of social media platforms people engage in now-a-days (note -  now-a-days in the decade of 2010-2020)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert sentence to lowercase\n",
    "\n",
    "# As a human, it is easy for us to understand that both of these two words have the same meaning (\"This\", \"this\"), however, for a computer they are two different words.\n",
    "print(\"This\" == \"this\") # False\n",
    "# Therefore, we must convert sentences to lowercase.\n",
    "\n",
    "sentence_low = sentence.lower()\n",
    "sentence2_low = sentence2.lower()\n",
    "\n",
    "print(\"\\n\" + sentence_low + \"\\n\")\n",
    "print(sentence2_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9ee2e",
   "metadata": {},
   "source": [
    "## Practical: NLTK Text Classification - Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1477006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Big', 'brown', 'fox', 'jumped', 'over', 'a', 'lazy', 'dog']\n",
      "['This', 'is', 'particularly', 'important', 'in', 'today', 's', 'world', 'where', 'we', 'are', 'swamped', 'with', 'unstructured', 'natural', 'language', 'data', 'on', 'the', 'variety', 'of', 'social', 'media', 'platforms', 'people', 'engage', 'in', 'now', 'a', 'days', 'note', 'now', 'a', 'days', 'in', 'the', 'decade', 'of', '2010', '2020']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Tokenise - tokenisation is the process of separating a chunk of text into smaller units called units, hence the word tokenisation\n",
    "# Tokenisation can be done by: (one words, two words, x words, one character, sub-words (ex: chunks of 4 characters,  chunks of x characters), etc)\n",
    "# In this case, we're tokenise by one word (so, each token correspond to one word)\n",
    "# Tokenisation essentially creates a vocabulary\n",
    "\n",
    "# In this case we're using a Regular Expression Tokeniser (RegexpTokenizer), \n",
    "# which means that we can give it some regular expression based on which it will tokenise the given text\n",
    "# Using this r'\\w+' regular expression will include words and exclude any punctuation.\n",
    "\n",
    "tokeniser = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokeniser.tokenize(sentence)\n",
    "print(tokens)\n",
    "tokens2 = tokeniser.tokenize(sentence2)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff02bf",
   "metadata": {},
   "source": [
    "Here, we used the original sentences and we haven't converted them to lowercase yet. The reason for it is because we are going to look at each individual technique atomically and then combine all of them together.\n",
    "\n",
    "We see that it removed punctuation, however, as a result certain words have been separated in a way that doens't seem right to us, for example: \n",
    "- \"today's\" => ['today', 's']\n",
    "- \"now-a-days\" => ['now', 'a', 'days']\n",
    "\n",
    "So, tokenisation is a way to segregate and get to a smaller unit which a model will be able to deal with. In certain scenarious, however, characters may be more important and so individual characters need to be extracted rather than words. \n",
    "\n",
    "Or, in other scenarious you may need to extract each token to include two words. \n",
    "\n",
    "So, these modifications be made. For our case, we wanted to extract each individual word so that we can understand the vocabulary of our documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c7ff64",
   "metadata": {},
   "source": [
    "## Practical: NLTK Text Classification - Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be00156b",
   "metadata": {},
   "source": [
    "**Stopwords removal** => is a technique for filtering words to remove all the non-useful words. Stopwords are words in any language which dont' add much meaning to the sentence and they can safely be ignored without sacrificing meaning of the sentence. Stopwords are also some of the most common words that appear in sentences, such as: \"the\", \"is\", \"at\", \"which\", \"on\" and you can think of them as filler words in a sentence. Sentence will still be able to transmit meaning without them. Let's look at applying stopwords removal with the help of an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55491a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9210d7ca",
   "metadata": {},
   "source": [
    "## Practical: NLTK Text Classification - Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4dd041",
   "metadata": {},
   "source": [
    "## Practical: NLTK Text Classification - Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5325f45",
   "metadata": {},
   "source": [
    "## Practical: NLTK Text Classification - Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db32f14",
   "metadata": {},
   "source": [
    "## Practical: Threading together all the preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9f561",
   "metadata": {},
   "source": [
    "## Practical: Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ccbd67",
   "metadata": {},
   "source": [
    "## Practical: NLP Chatbot Engine - Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3daa038",
   "metadata": {},
   "source": [
    "## Practical: Preparing data for machine learning text classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aad705",
   "metadata": {},
   "source": [
    "## Practical: Splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f322d5",
   "metadata": {},
   "source": [
    "## Practical: Decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa97093f",
   "metadata": {},
   "source": [
    "## Practical: Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b90ce",
   "metadata": {},
   "source": [
    "## Practical: Creating the NLP Chatbot Engine Function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
